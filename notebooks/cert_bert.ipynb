{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cert-bert.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xelav/Insider-Threat-CERT/blob/master/notebooks/cert_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6xD3mBh1o3"
      },
      "source": [
        "# Preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AgRFqtOWg-Y",
        "outputId": "1bdd238a-5586-49d5-b47e-80c5c4956238"
      },
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "root_dir = '/content/drive/My Drive/'\n",
        "\n",
        "%cd \"{root_dir}\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34c07MuxWief",
        "outputId": "db144197-508c-42be-b9f7-673a297794f6"
      },
      "source": [
        "repo_dir = \"/content/repo\"\n",
        "!git clone https://github.com/xelav/Insider-Threat-CERT \"$repo_dir\"\n",
        "\n",
        "import sys\n",
        "sys.path.append(repo_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '/content/repo'...\n",
            "remote: Enumerating objects: 300, done.\u001b[K\n",
            "remote: Counting objects: 100% (300/300), done.\u001b[K\n",
            "remote: Compressing objects: 100% (178/178), done.\u001b[K\n",
            "remote: Total 300 (delta 198), reused 205 (delta 120), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (300/300), 1.49 MiB | 4.87 MiB/s, done.\n",
            "Resolving deltas: 100% (198/198), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zS2z6WysWwLj",
        "outputId": "7e0f2182-c422-40bb-cfa2-36801155e8de"
      },
      "source": [
        "!pip install pytorch-ignite\n",
        "!pip install ipdb\n",
        "!pip install transformers\n",
        "!pip install tokenizers\n",
        "%pip install wandb==0.8.18 -q\n",
        "\n",
        "# import wandb\n",
        "# wandb.login()\n",
        "# wandb.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d3/640f70d69393b415e6a29b27c735047ad86267921ad62682d1d756556d48/pytorch_ignite-0.4.4-py3-none-any.whl (200kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 23.3MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 29.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 21.0MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 40kB 24.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 51kB 23.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 61kB 20.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 71kB 19.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 81kB 20.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 92kB 19.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 102kB 18.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 112kB 18.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 122kB 18.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 133kB 18.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 143kB 18.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 153kB 18.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 163kB 18.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 174kB 18.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 184kB 18.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 194kB 18.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch-ignite) (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.5)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.4\n",
            "Collecting ipdb\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/74/ebed6f1bda0dbd7641e19809882e060fb9e3fd1ff0e44c3071ec06c08078/ipdb-0.13.6.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (54.0.0)\n",
            "Collecting ipython>=7.17.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/43/6dbd0610550708fc418ad027fda97b5f415da9053749641654fdacfec93f/ipython-7.21.0-py3-none-any.whl (784kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 26.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/ee/08ceeb759c570bf96b4c636582ebf18c14c3c844a601b2e77b17f462aa6b/prompt_toolkit-3.0.17-py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 24.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.0.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3; sys_platform != \"win32\"->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.6-cp37-none-any.whl size=11383 sha256=9fe366049ec2e758e200015a9a6d5da257baa2585e87487f44ebf703681c5a3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/fc/6a/17cd402ab376f405e8ba3776cbad614a06909ba927d18790e0\n",
            "Successfully built ipdb\n",
            "\u001b[31mERROR: jupyter-console 5.2.0 has requirement prompt-toolkit<2.0.0,>=1.0.0, but you'll have prompt-toolkit 3.0.17 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipython~=5.5.0, but you'll have ipython 7.21.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "Successfully installed ipdb-0.13.6 ipython-7.21.0 prompt-toolkit-3.0.17\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/54/5ca07ec9569d2f232f3166de5457b63943882f7950ddfcc887732fc7fb23/transformers-4.3.3-py3-none-any.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 18.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=9a3c857654911e934b358bb2a5d12295c64a067a4ff27744e1608ac0c2d8be04\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.3.3\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 24.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 256kB 52.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 81kB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 163kB 51.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 133kB 61.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 184kB 60.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.8MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgpmeEcuWevq"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "import torch.utils.data\n",
        "from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator, Engine\n",
        "from ignite.metrics import Accuracy, Loss\n",
        "\n",
        "from src.models import InsiderClassifier, LSTM_Encoder, LSTM_Encoder_Topics, LSTM_Encoder_Numeric\n",
        "from src.params import get_params\n",
        "from src.dataset import CertDataset, create_data_loaders\n",
        "from src.lstm_trainer import *\n",
        "\n",
        "import ipdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVO1YA9EWevy"
      },
      "source": [
        "# Prepare run config\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-RYORHyWevz"
      },
      "source": [
        "# output_dir = Path(r'C:\\Users\\Mvideo\\Google Drive\\Datasets\\CERT_output')\n",
        "# answers_dir = Path(r\"C:/Users/Mvideo/Downloads/answers\")\n",
        "\n",
        "dataset_version = '4.2'\n",
        "\n",
        "output_dir = Path(f'{root_dir}/Datasets/CERT_output_v{dataset_version}')\n",
        "answers_dir = Path(f\"{root_dir}/Datasets/CERT/answers\")\n",
        "main_answers_file = answers_dir / \"insiders.csv\"\n",
        "\n",
        "assert(output_dir.is_dir())\n",
        "assert(answers_dir.is_dir())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqHif6AiZrwG"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertForSequenceClassification, BertConfig, \\\n",
        "    BertForMaskedLM, BertTokenizer, PreTrainedTokenizerBase, SpecialTokensMixin, \\\n",
        "    PreTrainedTokenizer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union, overload\n",
        "\n",
        "import pdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvdJUuhrdXTW"
      },
      "source": [
        "df = pd.read_parquet(output_dir / f'final_{dataset_version}.parquet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHV5FFOMg0q_"
      },
      "source": [
        "cert_dataset = CertDataset(df.action_id, df.malicious, positions=df.day_minutes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxR-VB-sGrhV"
      },
      "source": [
        "vocab_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbS7RgYsG1yc"
      },
      "source": [
        "class StubTokenizer(PreTrainedTokenizer):\n",
        "\n",
        "    def __init__(self, # vocab: Tuple[int],\n",
        "                 vocab_size,\n",
        "                 max_len=200,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.padding_side = 'right'\n",
        "        self._vocab_size = vocab_size + 5\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.unk_token=\"[UNK]\"\n",
        "        self.sep_token=\"[SEP]\"\n",
        "        self.pad_token=\"[PAD]\"\n",
        "        self.cls_token=\"[CLS]\"\n",
        "        self.mask_token=\"[MASK]\"\n",
        "        self.special_token_dict = {\n",
        "            self.pad_token: vocab_size + 1,\n",
        "            self.sep_token: vocab_size + 2,\n",
        "            self.cls_token: vocab_size + 3,\n",
        "            self.mask_token: vocab_size + 4,\n",
        "            self.unk_token: vocab_size + 5\n",
        "        }\n",
        "\n",
        "        # self.vocab = {token: ind for token, ind in zip(vocab, range(ind))}\n",
        "\n",
        "\n",
        "    def pad(self, examples, return_tensors = 'pt'):\n",
        "        assert return_tensors == 'pt'\n",
        "\n",
        "        input_ids = [ex['actions'] for ex in examples]\n",
        "\n",
        "        max_len = max([x.size for x in input_ids])\n",
        "        # if self.max_len is not None:\n",
        "        #     max_len = min(max_len, self.max_len)\n",
        "        # print(max_len)\n",
        "        \n",
        "        pad_token_id = self.convert_tokens_to_ids(self.pad_token)\n",
        "        \n",
        "        input_ids = [np.concatenate([x, np.ones(max_len - len(x), dtype=int) * pad_token_id]) for x in input_ids]\n",
        "        input_ids = torch.tensor(input_ids, dtype=torch.long)\n",
        "\n",
        "        position_ids = [np.concatenate([ex['positions'], np.ones(max_len - len(ex['actions']), dtype=int) * 1339]) for ex in examples]\n",
        "        position_ids = torch.tensor(position_ids, dtype=torch.long)\n",
        "         \n",
        "        padded_examples = {\n",
        "            'input_ids': input_ids,\n",
        "            'labels': torch.tensor([ex['targets'] for ex in examples], dtype=torch.long),\n",
        "            'position_ids': position_ids\n",
        "            }\n",
        "\n",
        "        return padded_examples\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens: Union[int, List[int]]) -> Union[int, List[int]]:\n",
        "        if tokens in self.special_token_dict:\n",
        "            return self.special_token_dict[tokens]\n",
        "        return int(tokens)\n",
        "        \n",
        "\n",
        "    @property\n",
        "    def vocab_size(self) -> int:\n",
        "        \"\"\"\n",
        "        :obj:`int`: Size of the base vocabulary (without the added tokens).\n",
        "        \"\"\"\n",
        "        return self._vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5GGY7q-cS25"
      },
      "source": [
        "# import random\n",
        "# import warnings\n",
        "# from dataclasses import dataclass\n",
        "# from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
        "\n",
        "# import torch\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "# from transformers.tokenization_utils_base import BatchEncoding, PreTrainedTokenizerBase\n",
        "\n",
        "\n",
        "# def _collate_batch(examples, tokenizer):\n",
        "#     \"\"\"Collate `examples` into a batch, using the information in `tokenizer` for padding if necessary.\"\"\"\n",
        "#     # Tensorize if necessary.\n",
        "#     if isinstance(examples[0], (list, tuple)):\n",
        "#         examples = [torch.tensor(e, dtype=torch.long) for e in examples]\n",
        "\n",
        "#     # Check if padding is necessary.\n",
        "#     length_of_first = examples[0].size(0)\n",
        "#     are_tensors_same_length = all(x.size(0) == length_of_first for x in examples)\n",
        "#     if are_tensors_same_length:\n",
        "#         return torch.stack(examples, dim=0)\n",
        "\n",
        "#     # If yes, check if we have a `pad_token`.\n",
        "#     if tokenizer._pad_token is None:\n",
        "#         raise ValueError(\n",
        "#             \"You are attempting to pad samples but the tokenizer you are using\"\n",
        "#             f\" ({tokenizer.__class__.__name__}) does not have a pad token.\"\n",
        "#         )\n",
        "\n",
        "#     # Creating the full tensor and filling it with our data.\n",
        "#     max_length = max(x.size(0) for x in examples)\n",
        "#     result = examples[0].new_full([len(examples), max_length], tokenizer.pad_token_id)\n",
        "#     for i, example in enumerate(examples):\n",
        "#         if tokenizer.padding_side == \"right\":\n",
        "#             result[i, : example.shape[0]] = example\n",
        "#         else:\n",
        "#             result[i, -example.shape[0] :] = example\n",
        "#     return result\n",
        "\n",
        "\n",
        "# @dataclass\n",
        "# class DataCollatorForLanguageModeling:\n",
        "#     \"\"\"\n",
        "#     Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n",
        "#     are not all of the same length.\n",
        "#     Args:\n",
        "#         tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n",
        "#             The tokenizer used for encoding the data.\n",
        "#         mlm (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
        "#             Whether or not to use masked language modeling. If set to :obj:`False`, the labels are the same as the\n",
        "#             inputs with the padding tokens ignored (by setting them to -100). Otherwise, the labels are -100 for\n",
        "#             non-masked tokens and the value to predict for the masked token.\n",
        "#         mlm_probability (:obj:`float`, `optional`, defaults to 0.15):\n",
        "#             The probability with which to (randomly) mask tokens in the input, when :obj:`mlm` is set to :obj:`True`.\n",
        "#     .. note::\n",
        "#         For best performance, this data collator should be used with a dataset having items that are dictionaries or\n",
        "#         BatchEncoding, with the :obj:`\"special_tokens_mask\"` key, as returned by a\n",
        "#         :class:`~transformers.PreTrainedTokenizer` or a :class:`~transformers.PreTrainedTokenizerFast` with the\n",
        "#         argument :obj:`return_special_tokens_mask=True`.\n",
        "#     \"\"\"\n",
        "\n",
        "#     tokenizer: PreTrainedTokenizerBase\n",
        "#     mlm: bool = True\n",
        "#     mlm_probability: float = 0.15\n",
        "\n",
        "#     def __post_init__(self):\n",
        "#         if self.mlm and self.tokenizer.mask_token is None:\n",
        "#             raise ValueError(\n",
        "#                 \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n",
        "#                 \"You should pass `mlm=False` to train on causal language modeling instead.\"\n",
        "#             )\n",
        "\n",
        "#     def __call__(\n",
        "#         self, examples: List[Union[List[int], torch.Tensor, Dict[str, torch.Tensor]]]\n",
        "#     ) -> Dict[str, torch.Tensor]:\n",
        "        \n",
        "#         ipdb.set_trace()\n",
        "#         # Handle dict or lists with proper padding and conversion to tensor.\n",
        "#         if isinstance(examples[0], (dict, BatchEncoding)):\n",
        "#             batch = self.tokenizer.pad(examples, return_tensors=\"pt\")\n",
        "#         else:\n",
        "#             batch = {\"input_ids\": _collate_batch(examples, self.tokenizer)}\n",
        "\n",
        "#         ipdb.set_trace()\n",
        "#         # If special token mask has been preprocessed, pop it from the dict.\n",
        "#         special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
        "#         if self.mlm:\n",
        "#             batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n",
        "#                 batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
        "#             )\n",
        "#         else:\n",
        "#             labels = batch[\"input_ids\"].clone()\n",
        "#             if self.tokenizer.pad_token_id is not None:\n",
        "#                 labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "#             batch[\"labels\"] = labels\n",
        "        \n",
        "#         ipdb.set_trace()\n",
        "#         return batch\n",
        "\n",
        "#     def mask_tokens(\n",
        "#         self, inputs: torch.Tensor, special_tokens_mask: Optional[torch.Tensor] = None\n",
        "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "#         \"\"\"\n",
        "#         Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "#         \"\"\"\n",
        "#         labels = inputs.clone()\n",
        "#         # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
        "#         probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "#         if special_tokens_mask is None:\n",
        "#             special_tokens_mask = [\n",
        "#                 self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "#             ]\n",
        "#             special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
        "#         else:\n",
        "#             special_tokens_mask = special_tokens_mask.bool()\n",
        "\n",
        "#         probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
        "#         masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "#         labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "#         # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "#         indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "#         inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "#         # 10% of the time, we replace masked input tokens with random word\n",
        "#         indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "#         random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "#         inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "#         # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "#         return inputs, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLFOcWUngm7B",
        "outputId": "ed3a4651-652b-49fd-cd21-98fdab2b2b1e"
      },
      "source": [
        "tokenizer = StubTokenizer(vocab_size=vocab_size)\n",
        "\n",
        "config = BertConfig(vocab_size=tokenizer.vocab_size,\n",
        "                    pad_token_id=tokenizer.convert_tokens_to_ids(tokenizer.pad_token),\n",
        "                    num_hidden_layers=2,\n",
        "                    max_position_embeddings=1440,\n",
        "                    )\n",
        "lm_model = BertForMaskedLM(config)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./masked_lm_enc\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_gpu_train_batch_size=32,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lm_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=cert_dataset\n",
        ")\n",
        "\n",
        "# trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0BQ2qd19jqU"
      },
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, max_seq_len = 80):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_embeddings = max_seq_len\n",
        "        \n",
        "        # create constant 'pe' matrix with values dependant on \n",
        "        # pos and i\n",
        "        pe = torch.zeros(max_seq_len, embedding_dim)\n",
        "        for pos in range(max_seq_len):\n",
        "            for i in range(0, embedding_dim, 2):\n",
        "                pe[pos, i] = \\\n",
        "                math.sin(pos / (10000 ** ((2 * i)/embedding_dim)))\n",
        "                pe[pos, i + 1] = \\\n",
        "                math.cos(pos / (10000 ** ((2 * (i + 1))/embedding_dim)))\n",
        "                \n",
        "        # pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, input):\n",
        "        \n",
        "        return F.embedding(input, self.pe)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3yWOxKu86Kh"
      },
      "source": [
        "emb = lm_model.bert.embeddings.position_embeddings.embedding_dim\n",
        "seq = lm_model.bert.embeddings.position_embeddings.num_embeddings\n",
        "\n",
        "lm_model.bert.embeddings.position_embeddings = PositionalEncoder(emb, seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RyGA4bqvG8oS",
        "outputId": "b351cc15-7b76-4318-c100-22d5797bb4d5"
      },
      "source": [
        "trainer = Trainer(\n",
        "    model=lm_model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=cert_dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='30981' max='30981' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30981/30981 2:00:37, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.362900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.305600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.294800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.299100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.299800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.291400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.285200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.285500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.262800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.257100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.249800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.240400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.244800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.235600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.235400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.235200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.224400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.224400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.221400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.215600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.217400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.218400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.213400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.208000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.203200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.205400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.200400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.202300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.203500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.207000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.202800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.201400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.199300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.199800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.198700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.199600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.195200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.195600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.201600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.198900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.194500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.194500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.191700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.191900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.196800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.194500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.196400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.192800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.190800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.192200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.195500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.195100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.192800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.196600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.188600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.193800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=30981, training_loss=0.22183569644058557, metrics={'train_runtime': 7237.4647, 'train_samples_per_second': 4.281, 'total_flos': 20478985429036680, 'epoch': 3.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffm7pqQnIVcS",
        "outputId": "a49a2f62-dca0-4e4c-d7f5-8d0e624bb1e6"
      },
      "source": [
        "%debug"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m(1913)\u001b[0;36membedding\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m   1911 \u001b[0;31m        \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1912 \u001b[0;31m        \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m-> 1913 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1914 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m   1915 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> u\n",
            "> \u001b[0;32m<ipython-input-27-6ed5ad5d2f0b>\u001b[0m(22)\u001b[0;36mforward\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m     18 \u001b[0;31m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pe'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     19 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     20 \u001b[0;31m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m     21 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\u001b[0;32m---> 22 \u001b[0;31m        \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0m\n",
            "ipdb> input\n",
            "tensor([[ 450,  928,  451,  465,  465,  472,  473,  474,  486,  486,  486,  489,\n",
            "          489,  505,  506,  510,  515,  517,  521,  539,  540,  541,  543,  548,\n",
            "          569,  573,  575,  576,  577,  585,  585,  588,  593,  602,  609,  632,\n",
            "          632,  632,  634,  635,  635,  644,  651,  652,  656,  659,  661,  665,\n",
            "          668,  672,  677,  679,  700,  704,  730,  752,  762,  763,  764,  764,\n",
            "          766,  770,  777,  779,  779,  782,  791,  796,  799,  805,  810,  815,\n",
            "          832,  839,  841,  865,  865,  865,  867,  873,  882,  883,  889,  889,\n",
            "          889,  890,  897,  904,  909,  915,  919,  919,  920,  920,  923,  925,\n",
            "          926,  589,  592,  595,  609,  610,  619,  627,  630,  693, 1339, 1339,\n",
            "         1339, 1339, 1339, 1339, 1339],\n",
            "        [ 468, 1151,  484,  488,  489,  490,  491,  491,  519,  526,  526,  539,\n",
            "          562,  574,  587,  587,  589,  589,  591,  592,  596,  615,  615,  619,\n",
            "          622,  640,  646,  666,  669,  671,  674,  674,  675,  675,  676,  676,\n",
            "          678,  680,  680,  681,  681,  686,  690,  690,  701,  701,  702,  702,\n",
            "          719,  738,  738,  741,  767,  818,  818,  818,  818,  818,  828,  836,\n",
            "          840,  856,  861,  868,  871,  876,  890,  894,  896,  896,  910,  912,\n",
            "          939,  939,  942,  946,  949,  955,  957,  962,  963,  963,  963,  964,\n",
            "          968,  976,  983,  984,  999, 1064, 1111, 1128, 1129, 1143, 1144, 1144,\n",
            "         1144, 1145, 1145, 1146, 1146, 1146,  903, 1054,  672,  673,  674,  677,\n",
            "          682,  888,  898,  907, 1143]])\n",
            "ipdb> input.shape\n",
            "torch.Size([2, 113])\n",
            "ipdb> self.pe\n",
            "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "           0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.6009e-01,  8.1525e-01,  ...,  1.0000e+00,\n",
            "           1.0491e-08,  1.0000e+00],\n",
            "         [ 9.0930e-01, -3.7260e-01,  9.4424e-01,  ...,  1.0000e+00,\n",
            "           2.0983e-08,  1.0000e+00],\n",
            "         ...,\n",
            "         [-9.6143e-01, -2.2030e-01, -4.0759e-02,  ...,  1.0000e+00,\n",
            "           1.5076e-05,  1.0000e+00],\n",
            "         [-7.5091e-01, -9.3147e-01,  7.9097e-01,  ...,  1.0000e+00,\n",
            "           1.5087e-05,  1.0000e+00],\n",
            "         [ 1.5000e-01, -8.2311e-01,  9.5687e-01,  ...,  1.0000e+00,\n",
            "           1.5097e-05,  1.0000e+00]]])\n",
            "ipdb> self.pe.shape\n",
            "torch.Size([1, 1440, 768])\n",
            "ipdb> exit\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}