{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inital processing of CERT dataset\n",
    "\n",
    "Output of this notebook is folder with processed `.csv` files with all features needed and with much smaller filesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_version = '6.2'\n",
    "\n",
    "answers_dir = Path(r\"C:\\Datasets\\CERT\\answers\")\n",
    "dataset_dir = Path(rf\"C:\\Datasets\\CERT\\r{dataset_version}\")\n",
    "\n",
    "main_answers_file = answers_dir / \"insiders.csv\"\n",
    "\n",
    "assert(answers_dir.is_dir())\n",
    "assert(dataset_dir.is_dir())\n",
    "assert(main_answers_file.is_file())\n",
    "\n",
    "output_dir = Path(f'C:/Datasets/CERT_output_v{dataset_version}/')\n",
    "assert(output_dir.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv(main_answers_file)\n",
    "main_df = main_df[main_df['dataset'] == dataset_version]\n",
    "\n",
    "malicious_users = main_df.user.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insider_files = list(answers_dir.glob(f'**/r{dataset_version}*.csv'))\n",
    "\n",
    "df_ls = []\n",
    "\n",
    "for filename in insider_files:\n",
    "    df = pd.read_csv(filename, names=list(range(13)))\n",
    "    df_ls.append(df)\n",
    "    \n",
    "df = pd.concat(df_ls, axis=0, ignore_index=True)"
   ]
  },
  {
   "source": [
    "Сайты, которые посещают инсайдеры:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(df[df[0] == 'http'][5].apply(lambda s: re.match('^https?://(www\\.)?([0-9\\-\\w\\.]+)?.+$', s).group(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cols = next(pd.read_csv(dataset_dir / 'device.csv', chunksize=10)).columns\n",
    "email_cols = next(pd.read_csv(dataset_dir / 'email.csv', chunksize=10)).columns\n",
    "file_cols = next(pd.read_csv(dataset_dir / 'file.csv', chunksize=10)).columns\n",
    "http_cols = next(pd.read_csv(dataset_dir / 'http.csv', chunksize=10)).columns\n",
    "logon_cols = next(pd.read_csv(dataset_dir / 'logon.csv', chunksize=10)).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Читаем датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_dir / 'logon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day'] = df['date'].dt.floor('D')\n",
    "\n",
    "# replaces df.groupby(['user', 'day']).pc.agg(pd.Series.mode)\n",
    "# it much more complicated but much, much faster\n",
    "# credit to: https://stackoverflow.com/a/57179083\n",
    "\n",
    "# result is exactly same as \"df.groupby('user').pc.agg(lambda x:x.value_counts().index[0])\" though\n",
    "\n",
    "most_common_pc = df\\\n",
    "    .groupby(['user', 'day', 'pc'])\\\n",
    "    .size()\\\n",
    "    .to_frame('count')\\\n",
    "    .reset_index()\\\n",
    "    .sort_values('count', ascending=False)\\\n",
    "    .drop_duplicates(subset=['user', 'day'])\\\n",
    "    .drop(columns=['count'])\\\n",
    "    .sort_values(['user', 'day'])\\\n",
    "    .groupby('user')\\\n",
    "    .pc\\\n",
    "    .agg(pd.Series.mode)\\\n",
    "    .rename('most_common_pc')\n",
    "most_common_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(most_common_pc, left_on='user', right_on='user', )\n",
    "df['is_usual_pc'] = df['most_common_pc'] == df['pc']\n",
    "\n",
    "is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
    "df['is_work_time'] = is_work_time\n",
    "\n",
    "df['subtype'] = df['activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(output_dir / 'logon_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = 'device'\n",
    "\n",
    "df = pd.read_csv(dataset_dir / f'{csv_name}.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "df = df.merge(most_common_pc, left_on='user', right_on='user', )\n",
    "df['is_usual_pc'] = df['most_common_pc'] == df['pc']\n",
    "\n",
    "is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
    "df['is_work_time'] = is_work_time\n",
    "\n",
    "df['subtype'] = df['activity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(output_dir / f'{csv_name}_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = 'file'\n",
    "\n",
    "df = pd.read_csv(dataset_dir / f'{csv_name}.csv', usecols=['date', 'user', 'pc', 'filename'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "df = df.merge(most_common_pc, left_on='user', right_on='user', )\n",
    "df['is_usual_pc'] = df['most_common_pc'] == df['pc']\n",
    "\n",
    "is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
    "df['is_work_time'] = is_work_time\n",
    "\n",
    "file_extensions = df.filename.str[-4:]\n",
    "df['subtype'] = file_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(output_dir / f'{csv_name}_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "csv_name = 'email'\n",
    "\n",
    "df = pd.read_csv(dataset_dir / f'{csv_name}.csv', usecols=['date', 'user', 'pc', 'to', 'cc', 'bcc', 'from'])\n",
    "df = df.fillna('')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from_domain = df['from'].str.extract('^.+@(.+$)')[0]\n",
    "is_external_from = from_domain == 'dtaa.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this lines takes ~10 mins\n",
    "# to_concated = df[['to', 'cc', 'bcc']].agg(';'.join, axis=1)\n",
    "\n",
    "# slighly slower but there is nice progress bar\n",
    "to_concated = df[['to', 'cc', 'bcc']].progress_apply(lambda x: ';'.join([x.to, x.cc, x.bcc]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes, it's horrible but this part is somewhat fast compared to the join part\n",
    "\n",
    "is_external_to = to_concated.progress_apply(\n",
    "    lambda x: any([re.match('^.+@(.+$)', e).group(1) != 'dtaa.com' for e in x.split(';') if e != ''])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_external = is_external_to | is_external_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "df = df.merge(most_common_pc, left_on='user', right_on='user', )\n",
    "df['is_usual_pc'] = df['most_common_pc'] == df['pc']\n",
    "\n",
    "is_work_time = (8 <= df.date.dt.hour) & (df.date.dt.hour < 17)\n",
    "df['is_work_time'] = is_work_time\n",
    "\n",
    "df['subtype'] = is_external"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype']].to_csv(output_dir / f'{csv_name}_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTTP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_hunting_websites = [\n",
    "    'careerbuilder.com',\n",
    "    'craiglist.org',\n",
    "    'indeed.com',\n",
    "    'job-hunt.org',\n",
    "    'jobhuntersbible.com',\n",
    "    'linkedin.com',\n",
    "    'monster.com',\n",
    "    'simplyhired.com',\n",
    "]\n",
    "\n",
    "hacktivist_websites = [\n",
    "    'actualkeylogger.com',\n",
    "    'best-spy-soft.com',\n",
    "    'dailykeylogger.com',\n",
    "    'keylogpc.com',\n",
    "    'refog.com',\n",
    "    'relytec.com',\n",
    "    'softactivity.com',\n",
    "    'spectorsoft.com',\n",
    "    'webwatchernow.com',\n",
    "    'wellresearchedreviews.com',\n",
    "    'wikileaks.org'\n",
    "]\n",
    "\n",
    "filesharing_websites = [\n",
    "    '4shared.com'\n",
    "    'dropbox.com',\n",
    "    'fileserve.com',\n",
    "    'filefreak.com',\n",
    "    'filestube.com',\n",
    "    'megaupload.com',\n",
    "    'thepiratebay.org'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_dir / 'http.csv') as f:\n",
    "    for count, l in tqdm(enumerate(f)):\n",
    "        pass\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast substitute for previous cell\n",
    "# count = 28434424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 500000\n",
    "\n",
    "df_iter = pd.read_csv(dataset_dir / 'http.csv', chunksize=CHUNK_SIZE, usecols=['date', 'user', 'pc', 'url'])\n",
    "# (output_dir / 'http_preprocessed.csv').unlink()\n",
    "first_it = True\n",
    "mode = 'w'\n",
    "\n",
    "for http_df in tqdm(df_iter, total=ceil(count / CHUNK_SIZE)):\n",
    "    http_df['date'] = pd.to_datetime(http_df.date, format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "    site_names = http_df['url'].apply(lambda s: re.match('^https?://(www)?([0-9\\-\\w\\.]+)?.+$', s).group(2))\n",
    "    http_df['site_name'] = site_names\n",
    "    \n",
    "    http_df['subtype'] = 0\n",
    "    http_df.loc[site_names.isin(job_hunting_websites), 'subtype'] = 1\n",
    "    http_df.loc[site_names.isin(hacktivist_websites), 'subtype'] = 2\n",
    "    http_df.loc[site_names.isin(filesharing_websites), 'subtype'] = 3\n",
    "    \n",
    "    http_df = http_df.merge(most_common_pc, left_on='user', right_on='user', )\n",
    "    http_df['is_usual_pc'] = http_df['most_common_pc'] == http_df['pc']\n",
    "\n",
    "    is_work_time = (8 <= http_df.date.dt.hour) & (http_df.date.dt.hour < 17)\n",
    "    http_df['is_work_time'] = is_work_time\n",
    "    \n",
    "    http_df.to_csv(output_dir / 'http_preprocessed.csv', header=first_it, index=False,\n",
    "                   mode=mode, columns=['date', 'user', 'is_usual_pc', 'is_work_time', 'subtype', 'site_name'])\n",
    "    first_it = False\n",
    "    mode = 'a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LDAP_dir = dataset_dir / 'LDAP'\n",
    "assert LDAP_dir.is_dir()\n",
    "ldap_columns = ['role', 'business_unit', 'functional_unit', 'department', 'team']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ldap_df_list = []\n",
    "\n",
    "# for file_path in LDAP_dir.glob('*.csv'):\n",
    "#     year, month = file_path.stem.split('-')\n",
    "#     ldap_df = pd.read_csv(file_path)\n",
    "    \n",
    "\n",
    "#     ldap_df = ldap_df[ldap_columns + ['user_id']]\n",
    "#     ldap_df['year'] = int(year)\n",
    "#     ldap_df['month'] = int(month)\n",
    "    \n",
    "#     ldap_df_list.append(ldap_df)\n",
    "    \n",
    "# ldap_df = pd.concat(ldap_df_list)\n",
    "# del ldap_df_list\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = ldap_df.user_id.unique()\n",
    "for user in users:\n",
    "    user_df = ldap_df[ldap_df['user_id'] == user]\n",
    "    for col in ldap_columns:\n",
    "        assert user_df[col].unique().shape[0] == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means users in CERT 4.2 do not change their units, departments, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldap_df = pd.read_csv(LDAP_dir / '2009-12.csv')\n",
    "for col in ldap_columns:\n",
    "    ldap_df[col] = ldap_df[col].astype('category')\n",
    "ldap_df = ldap_df[['user_id'] + ldap_columns]\n",
    "\n",
    "ldap_df.to_csv(output_dir / 'LDAP.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}