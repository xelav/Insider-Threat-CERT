{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Начало работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from math import ceil\n",
    "from itertools import chain\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Идеи для фичей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_dir = Path(r\"E:\\Datasets\\cert-answers\")\n",
    "dataset_dir = Path(r\"C:\\Users\\admin\\Downloads\\r4.2\\r4.2\")\n",
    "\n",
    "main_answers_file = answers_dir / \"insiders.csv\"\n",
    "\n",
    "assert(answers_dir.is_dir())\n",
    "assert(dataset_dir.is_dir())\n",
    "assert(main_answers_file.is_file())\n",
    "\n",
    "output_dir = Path(r'C:\\Users\\admin\\Google Drive\\Datasets\\CERT_output')\n",
    "assert(output_dir.is_dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_cols = next(pd.read_csv(dataset_dir / 'device.csv', chunksize=10)).columns\n",
    "email_cols = next(pd.read_csv(dataset_dir / 'email.csv', chunksize=10)).columns\n",
    "file_cols = next(pd.read_csv(dataset_dir / 'file.csv', chunksize=10)).columns\n",
    "http_cols = next(pd.read_csv(dataset_dir / 'http.csv', chunksize=10)).columns\n",
    "logon_cols = next(pd.read_csv(dataset_dir / 'logon.csv', chunksize=10)).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем единный словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_file_lines(file):\n",
    "    with open(file) as f:\n",
    "        for count, _ in enumerate(f):\n",
    "            pass\n",
    "    return count\n",
    "\n",
    "def collect_vocabulary(csv_name, chunk_size=100000):\n",
    "    \n",
    "    line_count = count_file_lines(dataset_dir / f'{csv_name}.csv')\n",
    "    \n",
    "    df_iter = pd.read_csv(dataset_dir / f'{csv_name}.csv', usecols=['date', 'user', 'content'], chunksize=chunk_size)\n",
    "    \n",
    "    result_set = set()\n",
    "\n",
    "    for df in tqdm(df_iter, total=(ceil(line_count / chunk_size))):\n",
    "        df['content'] = df['content'].str.lower().str.split()\n",
    "        result_set = result_set.union(*map(set, df['content']))\n",
    "        \n",
    "    return result_set\n",
    "\n",
    "# email_set = collect_vocabulary('email', chunk_size=500000)\n",
    "# file_set = collect_vocabulary('file', chunk_size=500000)\n",
    "# http_set = collect_vocabulary('http', chunk_size=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel, nmf\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaModel\n",
    "\n",
    "def chunk_iterator(filename, chunk_size=10000):\n",
    "    for chunk in tqdm(pd.read_csv(filename, chunksize=chunk_size)):\n",
    "        for document in chunk['content'].str.lower().str.split().values:\n",
    "            yield document\n",
    "            \n",
    "def bow_chunk_iterator(filenames, dictionary, chunk_size=10000):\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        for chunk in tqdm(pd.read_csv(filename, chunksize=chunk_size)):\n",
    "            for document in chunk['content'].str.lower().str.split().values:\n",
    "                yield dictionary.doc2bow(document)\n",
    "                \n",
    "def tfidf_chunk_iterator(filenames, dictionary, tfidf, chunk_size=10000):\n",
    "    for filename in filenames:\n",
    "        print(filename)\n",
    "        for chunk in tqdm(pd.read_csv(filename, chunksize=chunk_size)):\n",
    "            for document in chunk['content'].str.lower().str.split().values:\n",
    "                yield tfidf[dictionary.doc2bow(document)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [01:35,  2.12s/it]\n",
      "2844it [1:40:11,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "df_dict = Dictionary(chunk_iterator(dataset_dir / 'email.csv'))\n",
    "df_dict.add_documents(chunk_iterator(dataset_dir / 'file.csv'))\n",
    "df_dict.add_documents(chunk_iterator(dataset_dir / 'http.csv'))\n",
    "\n",
    "df_dict.save((output_dir / 'content_dictionary.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = Dictionary.load((output_dir / 'content_dictionary.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [07:33,  1.72s/it]\n",
      "45it [01:05,  1.46s/it]\n",
      "2844it [2:04:00,  2.62s/it]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfModel(\n",
    "    bow_chunk_iterator([\n",
    "        dataset_dir / 'email.csv',\n",
    "        dataset_dir / 'file.csv',\n",
    "        dataset_dir / 'http.csv'\n",
    "    ], df_dict))\n",
    "\n",
    "tfidf.save((output_dir / 'tfidf_model.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfModel.load((output_dir / 'tfidf_model.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Downloads\\r4.2\\r4.2\\email.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [36:19,  8.29s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Downloads\\r4.2\\r4.2\\file.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [06:20,  8.45s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Downloads\\r4.2\\r4.2\\http.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2844it [6:22:18,  8.07s/it]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1c61dd18f1c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdataset_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'file.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mdataset_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'http.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     ], df_dict, tfidf))\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mnmf_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dir\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;34m'nmf_model.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_posix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\gensim\\models\\nmf.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, chunksize, passes, kappa, minimum_probability, w_max_iter, w_stop_condition, h_max_iter, h_stop_condition, eval_every, normalize, random_state)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\gensim\\models\\nmf.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \"\"\"\n\u001b[0;32m    579\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_W\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_setup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[0mchunk_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python36\\lib\\site-packages\\gensim\\models\\nmf.py\u001b[0m in \u001b[0;36m_setup\u001b[1;34m(self, corpus)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mfirst_doc_it\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtee\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m             \u001b[0mfirst_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_doc_it\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[0mfirst_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus2csc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfirst_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_doc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_tokens\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nmf_model = nmf.Nmf(\n",
    "    tfidf_chunk_iterator([\n",
    "        dataset_dir / 'email.csv',\n",
    "        dataset_dir / 'file.csv',\n",
    "        dataset_dir / 'http.csv'\n",
    "    ], df_dict, tfidf,\n",
    "    ),\n",
    "    num_topics=100\n",
    ")\n",
    "\n",
    "nmf_model.save((output_dir / 'nmf_model.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Downloads\\r4.2\\r4.2\\email.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [34:12,  7.80s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Downloads\\r4.2\\r4.2\\file.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [05:34,  7.44s/it]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\Downloads\\r4.2\\r4.2\\http.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2844it [7:28:07,  9.45s/it]\n"
     ]
    }
   ],
   "source": [
    "lda_model = LdaModel(\n",
    "    tfidf_chunk_iterator([\n",
    "        dataset_dir / 'email.csv',\n",
    "        dataset_dir / 'file.csv',\n",
    "        dataset_dir / 'http.csv'\n",
    "    ], df_dict, tfidf,\n",
    "    ),\n",
    "    num_topics=100\n",
    ")\n",
    "\n",
    "lda_model.save((output_dir / 'lda_model.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LdaModel.load((output_dir / 'lda_model.pkl').as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_content(filename, dictionary, model, chunk_size = 10000, postfix='_lda_content'):\n",
    "    out_file = output_dir / (filename.stem + postfix + '.csv')\n",
    "    assert(not out_file.is_file())\n",
    "    for chunk in tqdm(pd.read_csv(filename, usecols=['id', 'content'], chunksize=chunk_size)):\n",
    "        chunk['content'] = chunk['content']\\\n",
    "            .str.lower()\\\n",
    "            .str.split()\\\n",
    "            .apply(lambda doc: model[dictionary.doc2bow(doc)])\n",
    "\n",
    "        chunk.to_csv(out_file, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "263it [4:04:18, 55.74s/it]\n"
     ]
    }
   ],
   "source": [
    "process_content(dataset_dir / 'email.csv', df_dict, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [43:19, 57.78s/it]\n"
     ]
    }
   ],
   "source": [
    "process_content(dataset_dir / 'file.csv', df_dict, lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credit to https://stackoverflow.com/a/53135031\n",
    "\n",
    "def parallelize(data, func, num_of_processes=8):\n",
    "    data_split = np.array_split(data, num_of_processes)\n",
    "    pool = Pool(num_of_processes)\n",
    "    data = pd.concat(pool.map(func, data_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return data\n",
    "\n",
    "def run_on_subset(func, data_subset):\n",
    "    return data_subset.apply(func, axis=1)\n",
    "\n",
    "def parallelize_on_rows(data, func, num_of_processes=8):\n",
    "    return parallelize(data, partial(run_on_subset, func), num_of_processes)\n",
    "\n",
    "def process_content_parallelized(filename, dictionary, model, chunk_size = 10000, postfix='_lda_content'):\n",
    "    out_file = output_dir / (filename.stem + postfix + '.csv')\n",
    "    assert(not out_file.is_file())\n",
    "    for chunk in tqdm(pd.read_csv(filename, usecols=['id', 'content'], chunksize=chunk_size)):\n",
    "    \n",
    "        chunk['content'] = parallelize_on_rows(\n",
    "            chunk['content'].str.lower().str.split()\n",
    "            , lambda doc: model[dictionary.doc2bow(doc)])\n",
    "\n",
    "        chunk.to_csv(out_file, mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2844it [38:47:50, 49.11s/it]\n"
     ]
    }
   ],
   "source": [
    "process_content(dataset_dir / 'http.csv', df_dict, lda_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
